{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Cleaning Data\n",
    "\n",
    "The data I am using is of **daily PM2.5 and AQI** levels at test sites across **California** from **2010-2018**. This notebook combines all the AQI files sourced from [the EPA](https://www.epa.gov/outdoor-air-quality-data/download-daily-data). It then cleans the data by dealing sorting, dealing with NaN values, and consolidating \"duplicate\" rows (measurements that were at the same test site on the same day).\n",
    "\n",
    "**Warning!** Running this notebook takes me 2-3 hours. You'll also need to download all the EPA data beforehand (see instructions in cell under \"Read in Data\"). No need to run this notebook if you have the pickle file already (which is included in the zip file).\n",
    "\n",
    "---\n",
    "\n",
    "**Getting data from online (not implemented):**\n",
    "- [`requests` Scrape website requiring login (Article)](http://kazuar.github.io/scraping-tutorial/)\n",
    "- [`requests` Login and download specific file w/ parameters (SO)](https://stackoverflow.com/questions/45107839/python-login-and-download-specific-file-from-website)\n",
    "- [`urllib` `mechanize` Using Python to sign into website, fill in a form, then sign out (SO)](https://stackoverflow.com/questions/8560959/using-python-to-sign-into-website-fill-in-a-form-then-sign-out)\n",
    "- [`mechanize` Fill online form (Blog)](https://www.thetaranights.com/fill-online-form-using-python/)\n",
    "- [`urllib` `requests` `mechanize` How to submit a web form (Article)](https://www.blog.pythonlibrary.org/2012/06/08/python-101-how-to-submit-a-web-form/)\n",
    "- [`webbrowser` `requests` `BeautifulSoup` `Selenium` Web Scraping (Book)](https://automatetheboringstuff.com/chapter11/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data from 2010-2018. Change to range(2010,2020) if you have 2019 data too!\n",
    "daterange = range(2010, 2019)\n",
    "\n",
    "# Go to https://www.epa.gov/outdoor-air-quality-data/download-daily-data\n",
    "# and download PM2.5 data, year, California (for all years you want).\n",
    "# Filenames are 'CA[year]_dailyAQI.csv', so 'CA2018_dailyAQI.csv' for 2018.\n",
    "# Store in folder: Data > dailyAQI\n",
    "df_list = [pd.read_csv('Data/dailyAQI/CA'+str(i)+'_dailyAQI.csv') for i in daterange]\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print('Warning - this may take up to a minute.')\n",
    "df_all['Date'] = pd.to_datetime(df_all['Date'])\n",
    "\n",
    "print('Shape:', df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most recent date in dataset: {}'.format(df_all.Date.max().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_nan(df):\n",
    "    null_bool = df.isnull().any(axis=1) # all rows with NaN\n",
    "\n",
    "    for i, b in enumerate(null_bool):\n",
    "        if b: # NaN in this row\n",
    "            county = df['COUNTY'][i]\n",
    "\n",
    "            site, code, name = df['Site Name'][i], df['CBSA_CODE'][i], df['CBSA_NAME'][i]\n",
    "\n",
    "            # Change NaNs to meaningful info\n",
    "            if isinstance(site, float) and np.isnan(site):\n",
    "                df.at[i, 'Site Name'] = county + ' (ID:' + str(df['Site ID'][i]) + ')'\n",
    "            if isinstance(code, float) and np.isnan(code):\n",
    "                df.at[i, 'CBSA_CODE'] = 0\n",
    "            if isinstance(name, float) and np.isnan(name):\n",
    "                df.at[i, 'CBSA_NAME'] = county + ', CA'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = remove_nan(df_all)\n",
    "df_all = df_all.sort_values(['Site ID','Date']) # Sort by Site, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN check:')\n",
    "df_all[df_all['COUNTY'] == 'Alpine'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Lebec!\n",
    "Lebec has two sets of data, one in Kern county and one in Los Angeles county. I'm just updating its site name to also include the site ID (so we can distinguish them).\n",
    "\n",
    "#### Distinguish duplicate site names (not sure how to do this more generally yet)\n",
    "Some sites are in more than one county (?!). In order to distinguish them, I'm updating these site names to also include their site ID, like when I updated NaN site names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORK IN PROGRESS...\n",
    "def update_dup_sites(df):\n",
    "    sites = np.sort(df['Site Name'].unique())\n",
    "    \n",
    "    for site in sites:\n",
    "        county_list = df[df['Site Name'] == site]['COUNTY'].unique()\n",
    "        if len(county_list) > 1:\n",
    "            for county in county_list:\n",
    "                df_bool = (df['COUNTY'] == county) & (df['Site Name'] == site)\n",
    "                ID = str(df[df_bool]['Site ID'].iloc[0])\n",
    "                \n",
    "                new_site = site + ' (ID:' + ID + ')'\n",
    "                df.loc[df['Site Name'] == site, 'Site Name'] = new_site\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = 'Lebec (ID:60292009)' # new site name for Kern county's Lebec\n",
    "la = 'Lebec (ID:60379034)' # new site name for LA county's Lebec\n",
    "for i in range(len(df_all)):\n",
    "    if df_all['Site Name'][i] == 'Lebec':\n",
    "        if df_all['COUNTY'][i] == 'Kern':\n",
    "            df_all.at[i, 'Site Name'] = kern\n",
    "        else: # Los Angeles\n",
    "            df_all.at[i, 'Site Name'] = la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lebec check:')\n",
    "display(df_all[df_all['Site Name'] == kern][:2])\n",
    "display(df_all[df_all['Site Name'] == la][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Before condensing (duplicates):')\n",
    "df_all[df_all['Site Name'] == 'Los Angeles-North Main Street'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condense rows at same site\n",
    "Some sites have more than one tool that measures AQI! That means that thereâ€™s more than one data point per date at that site. I am averging the multiple AQIs here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_rows(df, div):\n",
    "    start = time.time()\n",
    "    \n",
    "    tot_count = 0 # how many rows condensed\n",
    "    row = len(df)-1 # row number. Going backwards to avoid index shift\n",
    "\n",
    "    # for progress printing\n",
    "    progress = 0\n",
    "    p_mod = len(df)//div\n",
    "    print('Expected final count - {}: {} (will be lower)'.format(div//10, div))\n",
    "\n",
    "    while(True):\n",
    "        count = 1 # number of rows of same site\n",
    "        PM25_sum = 0 # sum of PM2.5 at site\n",
    "        AQI_sum = 0 # sum AQI at site\n",
    "\n",
    "        while(df.iloc[row,2] == df.iloc[row-1, 2] and\n",
    "              df.iloc[row,0] == df.iloc[row-1, 0]): # while still same site and day...\n",
    "            PM25_sum += df.iloc[row,4]\n",
    "            AQI_sum += df.iloc[row,6]\n",
    "            count += 1\n",
    "            df = df.drop(df.iloc[row].name) # drop row\n",
    "\n",
    "            if row > 1: row -= 1\n",
    "            else: break\n",
    "\n",
    "        if count > 1:\n",
    "            # while loop did not add these to sum\n",
    "            PM25_sum += df.iloc[row,4]\n",
    "            AQI_sum += df.iloc[row,6]\n",
    "\n",
    "            df.iat[row,3] = 0 # Set POC to 0, since multiple\n",
    "            df.iat[row,4] = np.round(PM25_sum/count, 1) # average of PM2.5 at site\n",
    "            df.iat[row,6] = int(round(AQI_sum/count)) # average of AQIs at site\n",
    "            df.iat[row,10] = 0 # Set AQS_PARAMETER_CODE to 0, since multiple\n",
    "            df.iat[row,11] = '' # Set AQS_PARAMETER_DESC to empty, since multiple\n",
    "\n",
    "            tot_count += count-1\n",
    "\n",
    "        # Counter\n",
    "        if progress % (p_mod*10) == 0:\n",
    "            print('\\n{0:03d}: '.format(int(progress/(p_mod*10))), end='')\n",
    "        if progress % p_mod == 0:\n",
    "            print('{0:04d}...'.format(int(progress/p_mod)), end='')\n",
    "        progress += 1\n",
    "\n",
    "        if row > 1: row -= 1\n",
    "        else: break\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    end = time.time()\n",
    "    elps = round(end-start)\n",
    "\n",
    "    print('Time elapsed: {}h {}m {}s'.format(elps//3600, (elps//60)%60, elps%60))\n",
    "    print(tot_count, 'rows removed')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = condense_rows(df_all, 1500) # takes 2-3 hours on my computer!\n",
    "\n",
    "print('\\nAfter condensing (no duplicates):')\n",
    "df_all[df_all['Site Name'] == 'Los Angeles-North Main Street'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all = df_all.reset_index(drop=True)\n",
    "df_all = df_all.sort_values(['Site ID','Date']) # Sort again to be safe\n",
    "\n",
    "print('Final Check:')\n",
    "print(df_all.shape)\n",
    "df_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.to_pickle('Data/CA2010+_dailyAQI.pkl') # save as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append new data to file (not really sure if this works correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_new_data(*paths):\n",
    "    '''\n",
    "    Takes current master data and appends new data (EPA updated dataset).\n",
    "    If there are datapoints retroactively added (data from a date before the\n",
    "    latest date in the master data), it might not be added.\n",
    "    '''\n",
    "    \n",
    "    df_all = pd.read_pickle('Data/CA2010+_dailyAQI.pkl')\n",
    "    df_list = [pd.read_csv(path) for path in paths]\n",
    "    \n",
    "    df_new = pd.concat(df_list, ignore_index=True)\n",
    "    df_new = df_new[df_all['Date'] > df_all.Date.max()] # only new dates\n",
    "    df_new['Date'] = pd.to_datetime(df_new['Date'])\n",
    "    \n",
    "    df_new = remove_nan(df_new)\n",
    "    df_new = condense_rows(df_new, 200*len(df_list))\n",
    "    \n",
    "    df_new = pd.concat((df_all, df_new))\n",
    "    df_new = df_new.sort_values(['Site ID','Date'])\n",
    "    df_new = df_new.reset_index()\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append_new_data('Data/dailyAQI/CA2018_dailyAQI_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
